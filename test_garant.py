"""
Test scraping from garant.ru
"""
import requests
from bs4 import BeautifulSoup
import re

url = "https://base.garant.ru/12125267/"

print("="*80)
print("Testing garant.ru")
print("="*80)
print(f"\nüì° Fetching: {url}\n")

try:
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    
    response = requests.get(url, headers=headers, timeout=15)
    response.raise_for_status()
    
    print(f"‚úÖ Response: {response.status_code}")
    print(f"üìÑ Content length: {len(response.text)} bytes\n")
    
    soup = BeautifulSoup(response.text, 'html.parser')
    full_text = soup.get_text()
    
    # Search for article 12.8
    print("üîç Searching for article 12.8...")
    
    search_terms = ['–°—Ç–∞—Ç—å—è 12.8', '—Å—Ç–∞—Ç—å—è 12.8', '12.8']
    pos = -1
    
    for term in search_terms:
        pos = full_text.find(term)
        if pos != -1:
            print(f"‚úÖ Found '{term}' at position {pos}\n")
            break
    
    if pos != -1:
        # Extract section
        article_section = full_text[pos:pos+2500]
        
        print("="*80)
        print("ARTICLE 12.8 SECTION:")
        print("="*80)
        print(article_section)
        print("="*80)
        
        # Save to file for analysis
        with open('garant_article_12_8.txt', 'w', encoding='utf-8') as f:
            f.write(article_section)
        print("\nüíæ Saved to garant_article_12_8.txt")
        
        # Extract fine
        print("\nüí∞ Extracting fine...")
        fine_patterns = [
            r'(\d+)\s+—Ç—ã—Å—è—á —Ä—É–±–ª–µ–π',
            r'(\d+)\s+—Ç—ã—Å\.\s+—Ä—É–±–ª–µ–π',
            r'—à—Ç—Ä–∞—Ñ.*?(\d+)\s+—Ç—ã—Å—è—á',
            r'–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω—ã–π —à—Ç—Ä–∞—Ñ.*?(\d+)\s+—Ç—ã—Å—è—á',
            r'–Ω–∞–ª–æ–∂–µ–Ω–∏–µ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ —à—Ç—Ä–∞—Ñ–∞.*?(\d+)\s+—Ç—ã—Å—è—á',
        ]
        
        fine_found = False
        for pattern in fine_patterns:
            match = re.search(pattern, article_section, re.IGNORECASE)
            if match:
                amount = int(match.group(1)) * 1000
                print(f"   ‚úÖ Fine: {amount:,} ‚ÇΩ".replace(',', ' '))
                print(f"   Pattern used: {pattern}")
                fine_found = True
                break
        
        if not fine_found:
            print("   ‚ùå Could not extract fine")
        
        # Extract license suspension
        print("\nüö´ Extracting license suspension...")
        license_patterns = [
            r'–ª–∏—à–µ–Ω–∏–µ –ø—Ä–∞–≤–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è.*?–Ω–∞ —Å—Ä–æ–∫ –æ—Ç\s+(\d+\.?\d*)\s+–¥–æ\s+(\d+\.?\d*)\s+(–ª–µ—Ç|–≥–æ–¥–∞|–º–µ—Å—è—Ü–µ–≤)',
            r'–ª–∏—à–µ–Ω–∏–µ –ø—Ä–∞–≤–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è.*?–æ—Ç\s+(\d+\.?\d*)\s+–¥–æ\s+(\d+\.?\d*)\s+(–ª–µ—Ç|–≥–æ–¥–∞|–º–µ—Å—è—Ü–µ–≤)',
            r'–ª–∏—à–µ–Ω–∏–µ–º –ø—Ä–∞–≤–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è.*?–æ—Ç\s+(\d+\.?\d*)\s+–¥–æ\s+(\d+\.?\d*)\s+(–ª–µ—Ç|–≥–æ–¥–∞|–º–µ—Å—è—Ü–µ–≤)',
            r'–ª–∏—à–µ–Ω–∏–µ.*?–æ—Ç\s+(\d+\.?\d*)\s+–¥–æ\s+(\d+\.?\d*)\s+(–ª–µ—Ç|–≥–æ–¥–∞|–º–µ—Å—è—Ü–µ–≤)',
        ]
        
        license_found = False
        for pattern in license_patterns:
            match = re.search(pattern, article_section, re.IGNORECASE)
            if match:
                print(f"   ‚úÖ License suspension: {match.group(1)} - {match.group(2)} {match.group(3)}")
                print(f"   Pattern used: {pattern}")
                license_found = True
                break
        
        if not license_found:
            print("   ‚ùå Could not extract license suspension")
        
    else:
        print("‚ùå Article 12.8 not found")
        print("\nüìù Sample content (first 1500 chars):")
        print(full_text[:1500])

except Exception as e:
    print(f"‚ùå Error: {e}")
    import traceback
    traceback.print_exc()

print("\n‚úÖ Test completed!\n")
