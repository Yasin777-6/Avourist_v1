# ü§ñ AI Evaluates AI - Conversation Quality Testing

## Overview

This system uses **one AI to evaluate another AI's selling skills**. Instead of manually testing conversations in Telegram, an AI evaluator rates:

- **–£–±–µ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å (Persuasiveness)** - How convincing is the sales pitch?
- **–≠–º–ø–∞—Ç–∏—è (Empathy)** - Does the AI understand the client's problem?
- **–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª–∏–∑–º (Professionalism)** - How professional is the response?
- **–ü—Ä–∏–∑—ã–≤ –∫ –¥–µ–π—Å—Ç–≤–∏—é (Call to Action)** - How clear is the CTA?
- **–†–∞—Å—á–µ—Ç ROI (ROI Calculation)** - How convincing is the financial argument?

---

## üöÄ Quick Start

### **Option 1: Run with Real AI Evaluation (Recommended)**

```bash
# Set your API key
export DEEPSEEK_API_KEY="your-api-key-here"

# Run evaluation
python run_ai_evaluation.py
```

### **Option 2: Run with Mock Data (No API needed)**

```bash
# Test without API calls
python run_ai_evaluation.py --mock
```

### **Option 3: Run Pytest Tests**

```bash
# Run all AI evaluation tests
pytest tests/test_ai_conversation_quality.py -v -s

# Run specific test
pytest tests/test_ai_conversation_quality.py::TestAISellingSkills::test_pricing_conversation_quality -v -s
```

---

## üìä What Gets Evaluated

### **4 Test Scenarios:**

1. **Pricing ROI Calculation**
   - Tests: Financial persuasion, ROI calculation
   - Expected Score: 9+/10

2. **Urgent Case Handling**
   - Tests: Urgency creation, fast response
   - Expected Score: 8.5+/10

3. **Empathy & Professionalism**
   - Tests: Understanding client pain, professional tone
   - Expected Score: 8.5+/10

4. **Contract Closing**
   - Tests: Clear CTA, payment terms, trust building
   - Expected Score: 9+/10

---

## üìà Evaluation Criteria

Each conversation is scored on 5 dimensions (1-10):

| Criterion | What It Measures | Target Score |
|-----------|------------------|--------------|
| **–£–±–µ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å** | How persuasive is the argument? | 9+ |
| **–≠–º–ø–∞—Ç–∏—è** | Does AI show empathy? | 8+ |
| **–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª–∏–∑–º** | Professional language & structure | 9+ |
| **–ü—Ä–∏–∑—ã–≤ –∫ –¥–µ–π—Å—Ç–≤–∏—é** | Clear call to action? | 9+ |
| **–†–∞—Å—á–µ—Ç ROI** | Convincing financial calculation? | 9+ |

**Overall Score:** Average of all 5 criteria

---

## üéØ Example Output

```
================================================================================
                    AI CONVERSATION QUALITY EVALUATION
================================================================================

‚Ñπ One AI evaluates another AI's selling skills

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Scenario 1/4: Pricing ROI Calculation
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

Client: –°–∫–æ–ª—å–∫–æ —Å—Ç–æ–∏—Ç –∑–∞—â–∏—Ç–∞ –ø–æ —Å—Ç.12.8?

AI Bot Response:
üîç –ê–Ω–∞–ª–∏–∑ –≤–∞—à–µ–π —Å–∏—Ç—É–∞—Ü–∏–∏:

–°—Ç–∞—Ç—å—è: —á.1 —Å—Ç.12.8 –ö–æ–ê–ü –†–§
–ü–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è –±–µ–∑ –∑–∞—â–∏—Ç—ã:
‚Ä¢ –®—Ç—Ä–∞—Ñ: 30,000‚ÇΩ
‚Ä¢ –õ–∏—à–µ–Ω–∏–µ –ø—Ä–∞–≤: 1.5-2 –≥–æ–¥–∞
‚Ä¢ –†–∞—Å—Ö–æ–¥—ã –Ω–∞ —Ç–∞–∫—Å–∏: ~540,000‚ÇΩ –∑–∞ 18 –º–µ—Å—è—Ü–µ–≤
‚Ä¢ –†–æ—Å—Ç –û–°–ê–ì–û: +25,000‚ÇΩ

üí∞ –†–∞—Å—á–µ—Ç –≤—ã–≥–æ–¥—ã —Å —é—Ä–∏—Å—Ç–æ–º:

–°—Ç–æ–∏–º–æ—Å—Ç—å –∑–∞—â–∏—Ç—ã: 30,000‚ÇΩ
–®–∞–Ω—Å —É—Å–ø–µ—Ö–∞: 75%
–û–∂–∏–¥–∞–µ–º–∞—è —ç–∫–æ–Ω–æ–º–∏—è: 421,250‚ÇΩ

–ó–∞—â–∏—Ç–∞ –æ–∫—É–ø–∞–µ—Ç—Å—è –≤ 14 —Ä–∞–∑!

–ë–µ—Ä–µ–º –≤–∞—à–µ –¥–µ–ª–æ? –û—Ñ–æ—Ä–º–∏–º –¥–æ–≥–æ–≤–æ—Ä –∑–∞ 5 –º–∏–Ω—É—Ç.

ü§ñ AI Evaluator is analyzing...

================================================================================
EVALUATION RESULTS
================================================================================

Overall Score: 9.2/10

Detailed Scores:
  –£–±–µ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: 9/10
  –≠–º–ø–∞—Ç–∏—è: 8/10
  –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª–∏–∑–º: 9/10
  –ü—Ä–∏–∑—ã–≤ –∫ –¥–µ–π—Å—Ç–≤–∏—é: 10/10
  –†–∞—Å—á–µ—Ç ROI: 10/10

–°–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã:
  ‚úì –ß–µ—Ç–∫–∏–π —Ä–∞—Å—á–µ—Ç ROI —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ —Ü–∏—Ñ—Ä–∞–º–∏
  ‚úì –°–∏–ª—å–Ω—ã–π –ø—Ä–∏–∑—ã–≤ –∫ –¥–µ–π—Å—Ç–≤–∏—é (–æ–∫—É–ø–∞–µ—Ç—Å—è –≤ 14 —Ä–∞–∑)
  ‚úì –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —ç–º–æ–¥–∑–∏
  ‚úì –ü–æ–∫–∞–∑–∞–Ω—ã –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ —Ä–∞—Å—Ö–æ–¥—ã (—Ç–∞–∫—Å–∏, –û–°–ê–ì–û)

–°–ª–∞–±—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã:
  ‚úó –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —Å–æ—Ü–∏–∞–ª—å–Ω–æ–µ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ (–æ—Ç–∑—ã–≤—ã)
  ‚úó –ù–µ —É–ø–æ–º—è–Ω—É—Ç—ã —Å—Ä–æ–∫–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:
  –û—Ç–ª–∏—á–Ω–∞—è —Ä–∞–±–æ—Ç–∞! –î–æ–±–∞–≤–∏—Ç—å —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ —É—Å–ø–µ—à–Ω—ã—Ö –∫–µ–π—Å–æ–≤ –∏ —Å—Ä–æ–∫–æ–≤.

================================================================================

[... 3 more scenarios ...]

================================================================================
                        FINAL EVALUATION SUMMARY
================================================================================

Scenarios Evaluated: 4
Average Score: 8.90/10

‚úì GREAT! AI selling skills are strong

Individual Scores:
  üü¢ Pricing ROI Calculation: 9.2/10
  üü¢ Urgent Case Handling: 8.8/10
  üü° Empathy & Professionalism: 8.6/10
  üü¢ Contract Closing: 9.0/10

================================================================================
```

---

## üß™ Test Files

### **1. `tests/test_ai_conversation_quality.py`**

Comprehensive pytest test suite with:

- **TestAISellingSkills** - Tests persuasion, urgency, empathy
- **TestConversationFlow** - Tests full sales funnel
- **TestContentFormatting** - Tests HTML formatting, conciseness
- **TestAIvsAIComparison** - Compares different AI responses

**Run:**
```bash
pytest tests/test_ai_conversation_quality.py -v -s
```

### **2. `run_ai_evaluation.py`**

Standalone evaluation script with:

- Real AI evaluation using DeepSeek API
- Mock evaluation for testing without API
- Colored terminal output
- Detailed scoring breakdown

**Run:**
```bash
# With real AI
python run_ai_evaluation.py

# With mock data
python run_ai_evaluation.py --mock
```

---

## üìù How It Works

### **Step 1: AI Bot Generates Response**

```python
ai_response = """
üí∞ –°—Ç–æ–∏–º–æ—Å—Ç—å: 30,000‚ÇΩ
–ë–µ–∑ –∑–∞—â–∏—Ç—ã –ø–æ—Ç–µ—Ä—è–µ—Ç–µ 565,000‚ÇΩ
–ó–∞—â–∏—Ç–∞ –æ–∫—É–ø–∞–µ—Ç—Å—è –≤ 14 —Ä–∞–∑!
–ë–µ—Ä–µ–º –¥–µ–ª–æ?
"""
```

### **Step 2: AI Evaluator Analyzes Response**

```python
evaluator = AIEvaluator(deepseek_service)

evaluation = evaluator.evaluate_conversation(
    client_message="–°–∫–æ–ª—å–∫–æ —Å—Ç–æ–∏—Ç?",
    ai_response=ai_response,
    case_type="DUI",
    region="MOSCOW",
    status="WARM"
)
```

### **Step 3: Get Detailed Scores**

```python
{
    "—É–±–µ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å": 9,
    "—ç–º–ø–∞—Ç–∏—è": 8,
    "–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª–∏–∑–º": 9,
    "–ø—Ä–∏–∑—ã–≤_–∫_–¥–µ–π—Å—Ç–≤–∏—é": 10,
    "—Ä–∞—Å—á–µ—Ç_roi": 10,
    "–æ–±—â–∞—è_–æ—Ü–µ–Ω–∫–∞": 9.2,
    "—Å–∏–ª—å–Ω—ã–µ_—Å—Ç–æ—Ä–æ–Ω—ã": ["–ß–µ—Ç–∫–∏–π ROI", "–°–∏–ª—å–Ω—ã–π CTA"],
    "—Å–ª–∞–±—ã–µ_—Å—Ç–æ—Ä–æ–Ω—ã": ["–ù–µ—Ç –æ—Ç–∑—ã–≤–æ–≤"],
    "—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏": "–î–æ–±–∞–≤–∏—Ç—å –∫–µ–π—Å—ã"
}
```

---

## üéì Understanding Scores

### **9-10: Excellent**
- Strong persuasion
- Clear ROI calculation
- Professional formatting
- Compelling CTA

### **7-8: Good**
- Decent persuasion
- Some ROI calculation
- Professional tone
- Adequate CTA

### **5-6: Needs Improvement**
- Weak persuasion
- No ROI calculation
- Unprofessional tone
- Unclear CTA

### **Below 5: Poor**
- No persuasion
- No financial argument
- Unprofessional
- No CTA

---

## üîß Customization

### **Add New Test Scenarios**

Edit `run_ai_evaluation.py`:

```python
def create_test_scenarios():
    return [
        {
            "name": "Your Scenario Name",
            "client_message": "Client question",
            "ai_response": "AI response to evaluate",
            "case_type": "DUI",
            "region": "MOSCOW",
            "status": "WARM"
        },
        # Add more scenarios...
    ]
```

### **Adjust Evaluation Criteria**

Edit `tests/test_ai_conversation_quality.py`:

```python
EVALUATION_PROMPT = """
–û—Ü–µ–Ω–∏ –¥–∏–∞–ª–æ–≥ –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—è–º:

1. –£–±–µ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å (1-10)
2. –≠–º–ø–∞—Ç–∏—è (1-10)
3. –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª–∏–∑–º (1-10)
4. [Your custom criterion] (1-10)
...
"""
```

---

## üìä Performance Benchmarks

### **Target Metrics:**

- **Average Score:** 8.5+/10
- **Persuasiveness:** 9+/10
- **ROI Calculation:** 9+/10
- **Call to Action:** 9+/10

### **Current Performance:**

Based on test scenarios:

| Scenario | Score | Status |
|----------|-------|--------|
| Pricing ROI | 9.2/10 | ‚úÖ Excellent |
| Urgent Case | 8.8/10 | ‚úÖ Great |
| Empathy | 8.6/10 | ‚úÖ Great |
| Contract | 9.0/10 | ‚úÖ Excellent |
| **Average** | **8.9/10** | ‚úÖ **Great** |

---

## üêõ Troubleshooting

### **Issue: "DEEPSEEK_API_KEY not set"**

**Solution:**
```bash
# Option 1: Set in .env file
echo "DEEPSEEK_API_KEY=your-key-here" >> .env

# Option 2: Export in terminal
export DEEPSEEK_API_KEY="your-key-here"

# Option 3: Pass as argument
python run_ai_evaluation.py --api-key "your-key-here"

# Option 4: Use mock mode
python run_ai_evaluation.py --mock
```

### **Issue: "JSON parsing error"**

**Solution:** The AI evaluator sometimes returns non-JSON text. The code has fallback handling, but you can improve by:

1. Increasing temperature to 0.1 (more deterministic)
2. Adding more explicit JSON formatting instructions
3. Using regex to extract JSON from response

### **Issue: Tests take too long**

**Solution:**
```bash
# Run quick mock tests
python run_ai_evaluation.py --mock

# Or run specific pytest test
pytest tests/test_ai_conversation_quality.py::TestAISellingSkills::test_pricing_conversation_quality -v
```

---

## üöÄ Integration with CI/CD

### **GitHub Actions Example:**

```yaml
name: AI Quality Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: 3.9
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install pytest colorama
      - name: Run AI evaluation tests
        env:
          DEEPSEEK_API_KEY: ${{ secrets.DEEPSEEK_API_KEY }}
        run: |
          python run_ai_evaluation.py
      - name: Check score threshold
        run: |
          # Fail if average score < 8.0
          python -c "import sys; sys.exit(0 if score >= 8.0 else 1)"
```

---

## üìà Continuous Improvement

### **Track Scores Over Time:**

```bash
# Save evaluation results
python run_ai_evaluation.py > evaluation_results_$(date +%Y%m%d).txt

# Compare with previous results
diff evaluation_results_20251001.txt evaluation_results_20251013.txt
```

### **A/B Testing:**

```python
# Test two different prompts
response_a = agent_with_prompt_v1.process(lead, message)
response_b = agent_with_prompt_v2.process(lead, message)

eval_a = evaluator.evaluate_conversation(..., response_a, ...)
eval_b = evaluator.evaluate_conversation(..., response_b, ...)

# Compare scores
if eval_b['–æ–±—â–∞—è_–æ—Ü–µ–Ω–∫–∞'] > eval_a['–æ–±—â–∞—è_–æ—Ü–µ–Ω–∫–∞']:
    print("Prompt v2 is better!")
```

---

## ‚úÖ Success Criteria

Your AI selling skills are considered **production-ready** if:

- ‚úÖ Average score ‚â• 8.5/10
- ‚úÖ Persuasiveness ‚â• 9/10
- ‚úÖ ROI calculation ‚â• 9/10
- ‚úÖ Call to action ‚â• 9/10
- ‚úÖ No scenario scores below 7/10

---

## üéâ Benefits

### **Why AI Evaluates AI?**

1. **Consistency** - Same evaluation criteria every time
2. **Speed** - Evaluate 100s of conversations in minutes
3. **Objectivity** - No human bias
4. **Scalability** - Test every code change automatically
5. **Cost-Effective** - No need for human QA testers

### **vs Manual Testing:**

| Method | Time | Cost | Consistency |
|--------|------|------|-------------|
| Manual Telegram Testing | Hours | High | Low |
| AI Evaluation | Minutes | Low | High |

---

## üìö Further Reading

- **Prompt Engineering:** Improve AI responses by refining prompts
- **A/B Testing:** Compare different conversation strategies
- **Conversion Optimization:** Use evaluation scores to improve conversion rates

---

## ü§ù Contributing

To add new evaluation criteria:

1. Edit `AIEvaluator.EVALUATION_PROMPT` in `tests/test_ai_conversation_quality.py`
2. Add new test scenarios in `run_ai_evaluation.py`
3. Update scoring thresholds in test assertions
4. Run tests to verify: `pytest tests/test_ai_conversation_quality.py -v`

---

## üìû Support

If you encounter issues:

1. Check API key is set correctly
2. Try mock mode first: `python run_ai_evaluation.py --mock`
3. Review test output for specific errors
4. Check DeepSeek API status

---

**Ready to evaluate your AI's selling skills?**

```bash
python run_ai_evaluation.py
```

üöÄ **Let one AI judge another!**
